## 2018-08-03
理解逻辑，剖析原因
1.为什么需要seq2seq?
机器翻译RNN（many to many）解决不了
2.为什么需要attention？
翻译过程中注意力不一样，歌词生成中押韵
3.为什么需要BeamSearch？
歌词生成中重复‘我爱我爱我’
4.为什么需要bleu?
歌词生成中语句通顺
5.为什么需要分词？
词向量，词的就不是单独的一个字

## 2018-08-02
1.seq2seq程序（庞）进行分析，同时和‘吴亦凡’一起将整个程序的流程走了一遍
  发现几点：1.这个程序和昨天的那个char-rnn程序极其相似，但是代码采用的是结构化文件，属于tensorflow模型代码，很棒
           2.四部分分成四个文件，十分清晰，data_generator、model、train、predict
           3.发现此程序的输出那里loss、summary里面的summary没有拿去生成tensorboard（陆伊），即么有log保存
2.交叉熵的概念还不是特别懂，看了一下李宏毅的goodness of function
3.陆给的那个seq2seq代码试了一下，里面有一个jieba库和其他库，可能以后会用到
4.回顾了一下防止过拟合：
  增加数据
  正则化（后面加了一个w平方的平均，不断迭代让部分w为0）
  dropout（一开始就让部分w为0）
5.回顾了一下优化器
  SGD
  Momentum
  NAG
  Adagrad
  RMSprop
  Adadelta
  Adam

## 2018-08-01
1.seq2seq基本概念的学习
2.attention-based model基本概念的学习
3.char-rnn-cn的例子下载后进行测试
  rnn_cell代码部分小改后
  training：正常
  Sampling：异常 
            出现错误百度后说是作用域，但其实代码上有作用域。
            最后重新启动TensorFlow正常输出。

4.char-rnn-cn的例子代码进行学习重写



